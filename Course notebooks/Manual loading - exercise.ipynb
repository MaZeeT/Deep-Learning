{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading numerical data manually!\n",
    "\n",
    "In this notebook we will learn to load data manually.\n",
    "Wether it be numerical data, image data in the form of numpy arrays or csv files, the method is the same.\n",
    "Though we have done similar things previously in this course, this method is much simpler as we take advantage of the Sklearn library.\n",
    "\n",
    "We will use the simple Boston housing dataset to:\n",
    "\n",
    "- load it from a csv file\n",
    "- extract into features and labels (x and y)\n",
    "- normalize the data\n",
    "- split via sklearns model_selection api (much simplier than the methods shown previously in this course)\n",
    "- train a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the csv file \"housing\" in the same folder as this notebook.\n",
    "Then we can just use pandas to load the file into a dataframe.\n",
    "\n",
    "But the dataframe contains lots of information and methods which we don't need or want.\n",
    "Hence we extract the values into the variable dataset.\n",
    "\n",
    "Afterwards we only need to separate features from the labels and we are set.\n",
    "We have 14 columns in total.\n",
    "Meaning that the first 13 columns are our features, and the last column, the 14th, is our label describing the price of the house.\n",
    "The labels are also sometimes called the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "\n",
    "# extract only the values of the pandas frame.\n",
    "# in essence the dataset variable will now be like a numpy array\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "# notice the split. The labels are the last column in our csv file,\n",
    "#thus we can just set y to the last value in the array\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we use data which is either normalized for us, or image data in which case we know a pixel have a max value of 255 or a minumum value of 0.\n",
    "\n",
    "With other numerical data we have to manually normalize (sometimes called scale) the data ourself.\n",
    "\n",
    "In the code below, we make use of Sklearn's preporcessing api and the MinMaxScaler.\n",
    "This scaler find the minimum and maximum value of each feature such that we can scale the data correctly between 0 and 1.\n",
    "\n",
    "Don't worry about the reshape for now.\n",
    "\n",
    "I have already filled out the scaling for the x value.\n",
    "\n",
    "I want you to do the same for the y value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# we need to reshape the Y variable for its dimensions to fit the scaler\n",
    "Y=np.reshape(Y, (-1,1))\n",
    "\n",
    "# define the MinMax scalers\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# actually fit the scaler and transform the data based on the fit\n",
    "print(scaler_x.fit(X))\n",
    "xscale=scaler_x.transform(X)\n",
    "\n",
    "# FILL THIS OUT\n",
    "print()\n",
    "yscale=\n",
    "\n",
    "# set values\n",
    "X=xscale\n",
    "Y=yscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have used complex methods to split our data.\n",
    "No more i say!\n",
    "\n",
    "We can simply use Sklearn to do this for us!\n",
    "you are welcome to play around with the split percentage and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data on an 80/20 split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.20, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our network.\n",
    "\n",
    "But something is missing!\n",
    "I want you to create 2 layers inbetween the input and output.\n",
    "Theese needs to be dense layers with 64 nodes and the relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# 2 DENSE LAYERS HERE\n",
    "\n",
    "model.add(layers.Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can train and evaluate the model.\n",
    "But we are missing the validation data in the fit method.\n",
    "\n",
    "Insert this touple as we have done times before.\n",
    "Remember we have already split the data such that we have x_test and y_test already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=10,\n",
    "          epochs=200,\n",
    "          # VALIDATION DATA HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
