{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together - Simple Kfold method\n",
    "\n",
    "Last time we worked with KFOLD in order to validate our result, to insure that our splits were not just lucky, and to check if the model was actually generalizing.\n",
    "\n",
    "Today we will look at a much simpler setup for doing KFOLD validation.\n",
    "First we will use the MNIST digits dataset since it is a simple image based dataset that our CPU's can process.\n",
    "Second we will use the Sklearn library, which contains lots of terrific functions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the dataset.\n",
    "\n",
    "Notice that we have to finde the number of classes, and the input_shape since this information is not directly supplied to us from the mnist.load_data() function.\n",
    "\n",
    "Further more, we have to normalize the data by scaling each pixel to be within 0-1.\n",
    "Afterwards we use numpy magic to get the correct shape for the tensor.\n",
    "Don't worry if numpy doesn't make sense the first many times that you use it, sometimes keras needs the data to be in a specific order or shape in which we use numpy magic to make that happend.\n",
    "Many times this means to simply remove a dimension as in our example here.\n",
    "\n",
    "Last we process the labels (y axis) to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply define a network.\n",
    "This small network should do okay and will run on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train!\n",
    "Notice that the fit function is used with the validation_split parameter.\n",
    "\n",
    "In short, this is actually a Sklearn function which you can use manually before providing data to the fit function.\n",
    "Then we would simply provide the datasets with a validation tuple as in earlier noteboks:\n",
    "validation_data = (x_val, y_val)\n",
    "\n",
    "But lets allow keras to do this for us for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "54000/54000 [==============================] - 19s 347us/sample - loss: 0.3646 - accuracy: 0.8898 - val_loss: 0.0820 - val_accuracy: 0.9792\n",
      "Epoch 2/15\n",
      "54000/54000 [==============================] - 24s 440us/sample - loss: 0.1119 - accuracy: 0.9657 - val_loss: 0.0554 - val_accuracy: 0.9848\n",
      "Epoch 3/15\n",
      "54000/54000 [==============================] - 23s 434us/sample - loss: 0.0846 - accuracy: 0.9733 - val_loss: 0.0481 - val_accuracy: 0.9887\n",
      "Epoch 4/15\n",
      "54000/54000 [==============================] - 23s 432us/sample - loss: 0.0708 - accuracy: 0.9786 - val_loss: 0.0420 - val_accuracy: 0.9883\n",
      "Epoch 5/15\n",
      "54000/54000 [==============================] - 23s 433us/sample - loss: 0.0621 - accuracy: 0.9810 - val_loss: 0.0371 - val_accuracy: 0.9903\n",
      "Epoch 6/15\n",
      "54000/54000 [==============================] - 23s 435us/sample - loss: 0.0560 - accuracy: 0.9827 - val_loss: 0.0386 - val_accuracy: 0.9897\n",
      "Epoch 7/15\n",
      "54000/54000 [==============================] - 24s 436us/sample - loss: 0.0518 - accuracy: 0.9841 - val_loss: 0.0349 - val_accuracy: 0.9900\n",
      "Epoch 8/15\n",
      "54000/54000 [==============================] - 24s 440us/sample - loss: 0.0486 - accuracy: 0.9846 - val_loss: 0.0371 - val_accuracy: 0.9882\n",
      "Epoch 9/15\n",
      "54000/54000 [==============================] - 24s 442us/sample - loss: 0.0455 - accuracy: 0.9854 - val_loss: 0.0332 - val_accuracy: 0.9907\n",
      "Epoch 10/15\n",
      "54000/54000 [==============================] - 24s 441us/sample - loss: 0.0437 - accuracy: 0.9863 - val_loss: 0.0311 - val_accuracy: 0.9917\n",
      "Epoch 11/15\n",
      "54000/54000 [==============================] - 25s 464us/sample - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.0310 - val_accuracy: 0.9920\n",
      "Epoch 12/15\n",
      "54000/54000 [==============================] - 26s 476us/sample - loss: 0.0382 - accuracy: 0.9870 - val_loss: 0.0319 - val_accuracy: 0.9907\n",
      "Epoch 13/15\n",
      "54000/54000 [==============================] - 24s 440us/sample - loss: 0.0374 - accuracy: 0.9876 - val_loss: 0.0303 - val_accuracy: 0.9922\n",
      "Epoch 14/15\n",
      "54000/54000 [==============================] - 24s 441us/sample - loss: 0.0367 - accuracy: 0.9881 - val_loss: 0.0287 - val_accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "54000/54000 [==============================] - 24s 449us/sample - loss: 0.0351 - accuracy: 0.9888 - val_loss: 0.0280 - val_accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c646d2b408>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02563383817092981\n",
      "Test accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have trained and gotten a good result!\n",
    "\n",
    "Now heres the problem, we don't know if our model is actually generalizing or if it was just a lucky split!\n",
    "\n",
    "KFOLD to the rescue again!\n",
    "\n",
    "We can use a KFOLD function directly from Sklearn.\n",
    "The amount of splits we will use is 5 (being a 5-Kfold), and we want to randomize the data order before the split such that the data is not in any kind of specific order.\n",
    "\n",
    "Lastly we define the random_state to, in our case, 3.\n",
    "This number is like a seed and will insure that we can make repreduceable results.\n",
    "The integer used can be anything 1-65k or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# used to hold results from folds during training.\n",
    "acc_fold = []\n",
    "loss_fold = []\n",
    "\n",
    "# setup folds\n",
    "kfold = KFold(n_splits=5, random_state=3, shuffle=True)\n",
    "\n",
    "# initialize fold tracking\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply iterate over the folds with an index to the train and test set of the specific fold.\n",
    "\n",
    "Remember here that we have compressed alot of code into the for loop since we need to train a new model for each fold!\n",
    "\n",
    "Towards the end of the loop we simply have evaluations and scores that we report, and insert into our previously defined acc and loss arrays to summarize at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 153us/sample - loss: 0.0512 - accuracy: 0.9848\n",
      "Score for fold 1: loss of 0.05121044223786642; accuracy of 0.9848333597183228\n",
      "\n",
      "\n",
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 157us/sample - loss: 0.0651 - accuracy: 0.9811\n",
      "Score for fold 2: loss of 0.06511581431026571; accuracy of 0.981083333492279\n",
      "\n",
      "\n",
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 172us/sample - loss: 0.0449 - accuracy: 0.9862\n",
      "Score for fold 3: loss of 0.04494275243704517; accuracy of 0.9861666560173035\n",
      "\n",
      "\n",
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 158us/sample - loss: 0.0499 - accuracy: 0.9859\n",
      "Score for fold 4: loss of 0.04993980676587671; accuracy of 0.9859166741371155\n",
      "\n",
      "\n",
      "Train on 43200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "12000/12000 [==============================] - 2s 157us/sample - loss: 0.0504 - accuracy: 0.9847\n",
      "Score for fold 5: loss of 0.05040370714183276; accuracy of 0.984749972820282\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "for train, test in kfold.split(x_train,y_train):\n",
    "    \n",
    "    batch_size = 128\n",
    "    epochs = 5\n",
    "    \n",
    "    FoldModel = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #FoldModel.summary()\n",
    "    FoldModel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    History = FoldModel.fit(x_train[train], y_train[train], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=3)\n",
    "    \n",
    "    scores = FoldModel.evaluate(x_train[test], y_train[test])\n",
    "    print(f'Score for fold {fold_no}: {FoldModel.metrics_names[0]} of {scores[0]}; {FoldModel.metrics_names[1]} of {scores[1]}')\n",
    "    acc_fold.append(scores[1]*100)\n",
    "    loss_fold.append(scores[0])\n",
    "    \n",
    "    fold_no = fold_no + 1\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.05121044223786642 - Accuracy: 98.48333597183228%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.06511581431026571 - Accuracy: 98.1083333492279%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.04494275243704517 - Accuracy: 98.61666560173035%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.04993980676587671 - Accuracy: 98.59166741371155%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.05040370714183276 - Accuracy: 98.4749972820282%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 98.45499992370605 (+- 0.18231529507923663)\n",
      "> Loss: 0.052322504578577365\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_fold[i]} - Accuracy: {acc_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_fold)} (+- {np.std(acc_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have arrived at the end.\n",
    "\n",
    "The summary shows both the results for each fold, but also the overall performance when calculated over the whole dataset.\n",
    "\n",
    "In our case here, we can see that the model is most likely generalizing!\n",
    "\n",
    "Lastly, it should be mentioned that KFOLD is normally only used for validating results, not to develop the model architecture itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
