{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST exercise - Now with fashion!\n",
    "\n",
    "For this exercise we will use the more interesting MNIST dataset containing fashion articles collected from Zalando!\n",
    "The great thing about this dataset is that it uses a more complex set of features (fashion articles is harder to classify than simple numbers) without wildly increasing the computational power needed for us to use it!\n",
    "\n",
    "Though, for this exercise you are gonna build vital parts of the architecture yourself.\n",
    "Feel free to use code from the previous notebook, but do type the code in manually instead of copy-paste.\n",
    "Muscle memory does exist and helps a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of using that pesky old numbers dataset, mnist, we want to load \"fashion_mnist\" instead!\n",
    "this means that our first job is to replace the function call from \"keras.datasets.mnist.load_data()\" to use the \"fashion_mnist\" instead.\n",
    "\n",
    "Secondly, we want to normalize our images.\n",
    "Normally an imagedatagenerator will do this for us automatically, but here we will do it manually!\n",
    "Since pixels have a max value of 255, we can simply normalize with this value as the max.\n",
    "Remember this only needs to be done for the features (pixels in this case) which only exist in the x_train and x_test variables!\n",
    "\n",
    "Feel free to look at the previous MNIST notebook from today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets - CHANGE TO FASHION MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range - NORMALIZE HERE!\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Model\n",
    "\n",
    "Now we want to start developing the architecture for our network!\n",
    "Let first try with the exact same network as with our MNIST numbers dataset!\n",
    "\n",
    "Notice the definition of the model, which allows us to create a new variable with the name model without restarting the notebook each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here how we have made the validation split 20% instead of 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reached 90% accuracy and 0.27 loss. Not bad.\n",
    "Lets see if we can improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced ConV model\n",
    "\n",
    "For the next model to try out, we want to remove the second convolutional layer and maxpooling layer.\n",
    "Thus the model should only consist of:\n",
    "\n",
    "- input layer\n",
    "- a 32 dimensional Conv2D layer with a kernel size of (3, 3) and an relu activation\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a flattening layer\n",
    "- a dropout layer with a rate of 0.5\n",
    "- an output dense layer with the softmax activation\n",
    "\n",
    "Feel free to look at the first model we have defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.,\n",
    "        layers.,\n",
    "        layers.,\n",
    "        layers.,\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we reached an accuracy of 89% and a loss of 0.29. thus, worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased Dense Model\n",
    "\n",
    "Hmm, lets try to increase the amount of dense layers we have instead.\n",
    "\n",
    "Thus the model should only consist of:\n",
    "- input layer\n",
    "- a 32 dimensional Conv2D layer with a kernel size of (3, 3) and an relu activation\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a 64 dimensional Conv2D layer with a kernel size of (3, 3) and an relu activation\n",
    "- a MaxPooling2D layer with a pool size of (2, 2)\n",
    "- a flattening layer\n",
    "- a 64 node dense layer with the relu activation\n",
    "- a dropout layer with a rate of 0.5\n",
    "- an output dense layer with the softmax activation\n",
    "\n",
    "I have kept the convolutional layers, such that you only need to add the dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        # Dense layer here!\n",
    "        layers.,\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a minor decrease in loss from our original model!\n",
    "\n",
    "Seems like the first model was the best (same result as the last model, but significantly decreased complexity and thus training times)!\n",
    "Thus we use the exact same model as the MNIST digits dataset!\n",
    "This will be our selected model moving forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an architecture that seems to perform acceptably, we need to validate the results!\n",
    "\n",
    "Of course we turn towards KFold.\n",
    "\n",
    "Now, you have to define the KFold method, but as we don't have that much time we only want to create 3 folds!\n",
    "Remember to set the random_state and to shuffle the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "acc_fold = []\n",
    "loss_fold = []\n",
    "\n",
    "# setup folds - USE THE KFold FUNCTION IMPORTED ABOVE!\n",
    "kfold = \n",
    "\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for train, test in kfold.split(x_train,y_train):\n",
    "    \n",
    "    batch_size = 128\n",
    "    epochs = 5\n",
    "    \n",
    "    # DEFINE THE SELECTED MODEL HERE! - ONLY THE HIDDEN LAYERS NEEDS TO BE DEFINED\n",
    "    FoldModel = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            # Model to be defined here!\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    FoldModel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # FILL IN THE CORRECT ARRAY INDEX HERE FOR X AND Y + CHANGE THE VALIDATION SIZE TO 20 %\n",
    "    History = FoldModel.fit(x_train[train], y_train[train], batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=3)\n",
    "    \n",
    "    \n",
    "    # FILL IN THE CORRECT ARRAY INDEX HERE FOR X AND Y\n",
    "    scores = FoldModel.evaluate(x_train[test], y_train[test])\n",
    "    \n",
    "    print(f'Score for fold {fold_no}: {FoldModel.metrics_names[0]} of {scores[0]}; {FoldModel.metrics_names[1]} of {scores[1]}')\n",
    "    acc_fold.append(scores[1]*100)\n",
    "    loss_fold.append(scores[0])\n",
    "    \n",
    "    fold_no = fold_no + 1\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(\"Done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_fold[i]} - Accuracy: {acc_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_fold)} (+- {np.std(acc_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
